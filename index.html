<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Nobline Yoo</title>

    <meta name="author" content="Nobline Yoo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon"> -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Nobline Yoo
                </p>
                <p>Hello! My name is Nobline, and I'm helping develop the DARPA Triage Challenge, using computer vision for effective medical triage in high-noise, diverse environments. 
                </p>
                <p>
                    </p>
                <p>
                  I graduated from Princeton University with a BSE in Computer Science and certificates in Statistics & Machine Learning and Robotics. I had the priviledge of working with <a href="https://www.cs.princeton.edu/~olgarus/">Professor Olga Russakovsky</a> in the <a href="https://visualai.princeton.edu/">Visual AI Lab</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:nobliney@alumni.princeton.edu">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/NoblineYooCV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=JJUnE_gAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/unofgithub">GitHub</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>News</h2>
              <p>
                <ul>
                  <li><b>August 2023: </b> Paper accepted to <em>ICCV Workshop Proceedings (ROAD++ Workshop)</em>: <a href="https://openaccess.thecvf.com/content/ICCV2023W/ROAD++/papers/Yoo_Efficient_Self-Supervised_Human_Pose_Estimation_with_Inductive_Prior_Tuning_ICCVW_2023_paper.pdf">Efficient, Self-Supervised Human Pose Estimation with Inductive Prior Tuning</a></li>
                  <li><b>August 2023: </b> Extended abstract accepted to <em>ICCV Women in Computer Vision Workshop</em>: <a href="data/YooICCV2023.pdf">Efficient, Self-Supervised Human Pose Estimation with Inductive Prior Tuning</a></li>
                  <li><b>February 2023: </b> Invited to Sigma Xi Honor Society.</li>
                  <li><b>October 2022: </b> Invited to Tau Beta Pi (top 1/5 of senior engineer class).</li>  
                  <li><b>May 2022: </b> Won the Independent Work Award from Princeton's Center for Statistics and Machine Learning for my junior independent work advised by Professor Brian Kernighan (awarded to 3 out of 124 students).</li>        
                  <li><b>June 2021: </b> Extended abstract accepted to <em>CVPR 2021 Visual Question Answering Workshop</em>: <a href="https://drive.google.com/file/d/1fl3c2V4ziooCa_Ikq_GztuDw95839xLA/view">Point and Ask: Incorporating Pointing Into Visual Question Answering</a></li>
                  <li><b>June 2021: </b> Student speaker at Professor Olga Russakovsky's talk at CVPR Women in Computer Vision Workshop: <a href="https://drive.google.com/file/d/1utRiqvXwMDDd-GaBUvtEhLJdfED0HtrX/view?t=4s">Recorded Talk</a></li>
                  <li><b>2021: </b> Invited to Tau Beta Pi (top 1/8 of junior engineer class).</li> 
                </ul>
                </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<tr bgcolor="#ffffd0">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <img src='images/hpe_ipt.png' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://openaccess.thecvf.com/content/ICCV2023W/ROAD%2B%2B/html/Yoo_Efficient_Self-Supervised_Human_Pose_Estimation_with_Inductive_Prior_Tuning_ICCVW_2023_paper.html">
      <span class="papertitle">Efficient, Self-Supervised Human Pose Estimation with Inductive Prior Tuning</span>
    </a>
    <br>
	<strong>Nobline Yoo</strong>,
    <a href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a>
    <br>
	<em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</em>, 2023
    <br>
    <a href="https://openaccess.thecvf.com/content/ICCV2023W/ROAD++/papers/Yoo_Efficient_Self-Supervised_Human_Pose_Estimation_with_Inductive_Prior_Tuning_ICCVW_2023_paper.pdf">[paper]</a>
    <a href="data/YooICCV2023.pdf">[poster]</a>
    <a href="https://github.com/princetonvisualai/hpe-inductive-prior-tuning">[code]</a>
    <a href="https://mediacentral.princeton.edu/media/Human-level+reasoning+without+the+need+for+an+inhuman+learning+paradigmF%2C+Nobline+Yoo%2C+UG+%2723+%282272767%29/1_t5a4t3hz">[video]</a>
    <p></p>
    <p>
        We explore the pose estimation-reconstruction alignment task that is central to self-supervised learning in human pose estimation. 
        We show that inductive-prior tuning helps coordinate 
        pose learning alongside reconstruction and propose a new metric
        that measures limb length consistency in a no-labels setting. Our model outperforms the baseline using less than 30.3% the amount of training data.
    </p>
  </td>
</tr>          

<tr bgcolor="#ffffd0">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/pointingqa.png' width="160">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/abs/2011.13681">
        <span class="papertitle">Point and Ask: Incorporating Pointing into Visual Question Answering</span>
      </a>
      <br>
      <a href="https://arjun-mani.github.io/">Arjun Mani</a>,
      <strong>Nobline Yoo</strong>,
      <a href="https://scholar.google.com/citations?user=ue2n778AAAAJ">Will Hinthorn</a>,
      <a href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a>
      <br>
      <em>Poster spotlight at Visual Question Answering (VQA) workshop at CVPR</em>, 2021
      <br>
      <a href="https://arxiv.org/abs/2011.13681">[paper]</a>
      <a href="https://github.com/princetonvisualai/pointingqa">[code]</a>
      <p></p>
      <p>
        We introduce and motivate point input for VQA questions and propose benchmarks consisting of questions which require 
        point input (e.g. What color is this book?). We find point supervision is more effective than verbal disambiguation.
      </p>
    </td>
  </tr> 

</tbody></table>

<!-- RESEARCH PROJECTS -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <h2>Research Projects</h2>
    </td>
  </tr>
</tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<tr>
<td style="padding:20px;width:25%;vertical-align:middle">
<div class="one">
<img src='images/dtc.jpg' width="160">
</div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
<a href="https://triagechallenge.darpa.mil/index">
<span class="papertitle">Computer Vision for Medical Triage in High-Noise, Diverse Environments</span>
</a>
<br>
<strong>Nobline Yoo</strong>, <a href="https://www.jhuapl.edu/">Johns Hopkins University Applied Physics Laboratory</a>
<br>
<em>DARPA Triage Challenge (current work-in-progress)</em> 
<br>
<a href="https://triagechallenge.darpa.mil/index">[project page]</a>
<p></p>
<p>
Currently helping develop the DARPA Triage Challenge. We are working on developing computer vision models 
to detect hemorrhage and respiratory distress for effective medical triage in high-noise, diverse environments.
</p>
</td>
</tr>          

<tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/losses_point_nerf.png' height="160">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://drive.google.com/file/d/1yQlXWEVhJXllgsHo0X-XxVVum_FvRMej/view">
        <span class="papertitle">Perceptual Losses for Point-Based Neural Fields</span>
      </a>
      <br>
      <strong>Nobline Yoo</strong>,
      <a href="https://www.cs.princeton.edu/~fheide/">Felix Heide</a>
      <br>
        <em>Neural Rendering (Grad) Course Project</em>, Spring 2023
        <br>
      <a href="https://drive.google.com/file/d/1yQlXWEVhJXllgsHo0X-XxVVum_FvRMej/view">[paper]</a>
      <a href="https://github.com/unofgithub/cos526-perceptual-losses-point-based-nerf">[code]</a>
      <p></p>
      <p>
        We explore MS-SSIM and Canny edge-based losses for the novel view synthesis method proposed
        in <a href="https://dl.acm.org/doi/abs/10.1145/3550469.3555413">Zhang et al.</a> We show edge-based loss reduces 
        spot artifacts with high levels of detail reconstruction. On a real-world scene from the LLFF dataset, 
        we introduce a simple modification to Zhang et al. which yields a 4.68 point increase in PSNR.
      </p>
    </td>
  </tr>        

  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <img src='images/prefix_tuning.png' width="160">
    </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://drive.google.com/file/d/1Dtvst-zzubB1xsiQZEEkJpXdB7LsB4g5/view">
    <span class="papertitle">Prefix Tuning: Optimizing Large Language Models</span>
    </a>
    <br>
    Grace Wang*,
    <strong>Nobline Yoo*</strong>,
    Richard Zhu*,
    <a href="https://www.cs.princeton.edu/~karthikn/">Karthik Narasimhan</a>
    <br>
    <em>Natural Language Processing Course Project</em>, Spring 2022
    <br>
    <a href="https://drive.google.com/file/d/1Dtvst-zzubB1xsiQZEEkJpXdB7LsB4g5/view">[paper]</a>
    <a href="https://drive.google.com/file/d/1C2XxJPbO-K0L6pSuRF447WMP3HPl6HUk/view?usp=drive_link">[poster]</a>
    <a href="https://github.com/unofgithub/cos484-prefix-tuning">[code]</a>
    <p></p>
    <p>
        Reproduced paper from ACL | IJCNLP 2021 <a href="https://aclanthology.org/2021.acl-long.353/">Li and Liang</a>.
        Conducted four additional ablation studies on prefix tuning for LLMs. Noticed a threshold
prefix length (15 to 20) after which accuracy declines in a small dataset setting. Identified a
prefix initialization that led to 2.6, 4.0, 0.17 point increase in METEOR, ROUGE-L, and CIDEr
metrics from state of the art.
    </p>
    </td>
    </tr> 

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
      <img src='images/dighum_hci_cover.png' width="160">
      </div>
      </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://drive.google.com/file/d/1ZwGOYRIcTSx15NWctoTMSFhHjZKV5HtH/view">
    <span class="papertitle">Digital Humanities Tools as Sites of Human-Computer Interaction</span>
    </a>
    <br>
    <strong>Nobline Yoo</strong>,
    <a href="https://www.cs.princeton.edu/people/profile/bwk">Brian Kernighan</a>
    <br>
    <em>Junior Independent Work</em>, Spring 2022
    <br>
    <a href="https://drive.google.com/file/d/1ZwGOYRIcTSx15NWctoTMSFhHjZKV5HtH/view">[paper]</a>
    <a href="https://colab.research.google.com/drive/1Bw0gK7_6x6zFcfIU8LM3rj61WxUK3UFO?usp=sharing">[code]</a>
    <a href="https://cdh.princeton.edu/updates/2022/08/18/outstanding-undergrads-in-digital-humanities-nobline-yoo-23/">[spotlight]</a>
    <a href="https://csml.princeton.edu/news/projects-csml-students-showcase-innovative-thinking-and-diversity-disciplines">[award]</a>
    <p></p>
    <p>
        Created an analysis tool for the Chronicling America dataset, using word vector analysis and topic
        modeling, and identified digital humanities tools as sites of human-computer interaction, motivating the need for a new framework 
        to maximize tool adoption.
        <a href="https://csml.princeton.edu/news/projects-csml-students-showcase-innovative-thinking-and-diversity-disciplines">Awarded</a>
        the Independent Work Award by the Princeton Center for Statistics and Machine
        Learning and <a href="https://cdh.princeton.edu/updates/2022/08/18/outstanding-undergrads-in-digital-humanities-nobline-yoo-23/"></a>featured by the Princeton Center for Digital Humanities.
    </p>
    </td>
    </tr> 

</tbody></table>

<!-- MORE PROJECTS -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <h2>More Projects</h2>
    </td>
  </tr>
</tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  
<tr>
<td style="padding:20px;width:25%;vertical-align:middle">
<div class="one">
<img src="images/drone.gif" width="160">
</div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1qE0MCMg0Sp-0VhcTuFhjEhRDIOwaPWJw/view?usp=sharing">
<span class="papertitle">Drone: Navigating through Obstacles with Computer Vision</span>
</a>
<br>
<em>Introduction to Robotics</em>, Fall 2022
<br>
<a href="https://drive.google.com/file/d/1qE0MCMg0Sp-0VhcTuFhjEhRDIOwaPWJw/view?usp=drive_link">[video]</a>
<p></p>
<p>
In a group of four, we programmed a drone to navigate an obstacle course using computer vision for obstacle and destination recognition.
</p>
</td>
</tr>          

<tr>
<td style="padding:20px;width:25%;vertical-align:middle">
<div class="one">
<img src='images/ship.png' width="160">
</div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
<a href="https://drive.google.com/file/d/1eZeMn62pN-oSzhhks_h3XxS57Sp_XsYn/view?usp=sharing">
<span class="papertitle">"A ship, we see, is not just a ship on the sea."</span>
</a>
<br>
<strong>Nobline Yoo</strong>,
<a href="https://psychology.princeton.edu/people/adele-goldberg">Adele Goldberg</a>
<br>
<em>Psychology of Language Course Paper</em>, Spring 2021
<br>
<a href="https://drive.google.com/file/d/1eZeMn62pN-oSzhhks_h3XxS57Sp_XsYn/view?usp=sharing">[paper]</a>
<p></p>
<p>
A ship is a bowl on the ocean. But a ship also refers to "fortune," "opportunity," or even an "organization." 
We explore how the word "ship" has come to acquire its present set of meanings, as derived from its prototypical 
definition, focusing on the role that extended context plays in determining its place in our human mind map. Though unexplored 
in the essay, it would be very interesting to ponder the role of extended context in vision settings for creating a 
computer's "mind map" of objects and their relations with one another.
</p>
</td>
</tr> 

</tbody></table>

<!-- TEACHING/OUTREACH -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <h2>Teaching & Outreach</h2>
    </td>
  </tr>
</tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<tr>
<td style="padding:20px;width:25%;vertical-align:middle">
<div class="one">
<img src='images/ai4all.png' width="160">
</div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
<a href="https://ai4all.princeton.edu/">
<span class="papertitle">Princeton AI4ALL</span>
</a>
<br>
<em>Lead Instructor</em> (Research Instructor in Summer 2019, 2020, 2021. Lead Instructor in 2023).
<br>
<p></p>
<p>

Led and set vision for 11-person instructional team on machine learning curriculum development for high-school students from underrepresented 
backgrounds. Designed curriculum with focus on hands-on learning, social impact, and 
ethical design in four core projects: (1-2) computer vision for accessibility and rainforest conservation, 
(3) natural language processing for mental health, (4) robotics for environmental monitoring. Instructed cohorts of ~30 students each year.
</p>
</td>
</tr>          

<tr>
<td style="padding:20px;width:25%;vertical-align:middle">
<div class="one">
<img src='images/pu_logo.jpeg' width="160">
</div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">

<span class="papertitle"><a href="https://www.cs.princeton.edu/">Undergraduate Course TA</a> & <a href="https://mcgraw.princeton.edu/">Tutor</a></span>
<br>
<em>Undergraduate Course Grader, Princeton McGraw Center for Teaching & Learning Tutor</em> (Fall 2022, Spring 2023)
<br>
<p></p>
<p>
<strong>COS333: </strong> Graded semester-long projects for Advanced Programming Techniques (COS333).<br>
<strong>POL345, SML201, R: </strong> Mentored students in R programming, quantitative social science, and data science in 
individual and group environments. Employed various, formal teaching methods to tailor assistance to individuals to help solidify 
foundational knowledge and learn how to learn.
<br>
</p>
</td>
</tr> 

<tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <img src='images/swe.png' width="160">
    </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://swe.princeton.edu/">
    <span class="papertitle">Princeton Society of Women Engineers High School Engineering Colloquium</span>
    </a>
    <br>
    <em>Panelist</em>, 2023
    <br>
    <p></p>
    <p>
  Mentored high-school girls interested in STEM by answering questions about Princeton Engineering experience and giving advice for 
  paving a path forward in engineering.
    </p>
    </td>
    </tr>   

</tbody></table>

    <!-- <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='camp_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/camp.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/camp.png' width="160">
        </div>
        <script type="text/javascript">
          function camp_start() {
            document.getElementById('camp_image').style.opacity = "1";
          }

          function camp_stop() {
            document.getElementById('camp_image').style.opacity = "0";
          }
          camp_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://camp-nerf.github.io/">
          <span class="papertitle">CamP: Camera Preconditioning for Neural Radiance Fields</span>
        </a>
        <br>
        <a href="https://keunhong.com/">Keunhong Park</a>,
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <strong>Jonathan T. Barron</strong>,
        <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>
        <br>
        <em>SIGGRAPH Asia</em>, 2023
        <br>
        <a href="https://camp-nerf.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2308.10902">arXiv</a>
        <p></p>
        <p>
        Preconditioning based on camera parameterization helps NeRF and camera extrinsics/intrinsics optimize better together.
        </p>
      </td>
    </tr>            
            
          </tbody></table> -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Design and source code from <a href="https://jonbarron.info/">John Barron</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
